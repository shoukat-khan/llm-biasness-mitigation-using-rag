# LLM Bias Mitigation Using RAG

A comprehensive project demonstrating how Retrieval-Augmented Generation (RAG) can be used to mitigate gender bias in Large Language Models (LLMs).

## ğŸ¯ Project Overview

This project implements a dual-mode system that compares:
- **Biased Mode**: Raw LLM outputs without bias mitigation
- **Unbiased Mode**: RAG-enhanced outputs using a curated knowledge base

The system uses **FLAN-T5** (google/flan-t5-base) as the base model and demonstrates how injecting factual, anti-bias knowledge through RAG can significantly reduce harmful stereotypes in AI-generated content.

## ğŸ“‹ Features

- âœ… Complete RAG pipeline implementation
- âœ… Curated knowledge base with 77 anti-bias content files
- âœ… FAISS vector store for efficient semantic search
- âœ… Streamlit web interface with dual-mode comparison
- âœ… Comprehensive evaluation framework
- âœ… Complete documentation (3-part final report)

## ğŸ—ï¸ Architecture

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web Interface  â”‚  (Streamlit)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
Biased Mode  RAG Mode
    â”‚         â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚    â”‚         â”‚
    â”‚ Retrieval  FLAN-T5
    â”‚    â”‚         â”‚
    â”‚ FAISS    Knowledge
    â”‚ Index    Base
    â”‚
FLAN-T5
(No Context)
```

## ğŸ“ Repository Structure

```
rag_gender_bias_project/
â”œâ”€â”€ src/                    # Core source code
â”‚   â”œâ”€â”€ retrieval.py        # Retrieval module
â”‚   â”œâ”€â”€ rag_generator.py    # RAG generator
â”‚   â”œâ”€â”€ biased_generator.py # Biased baseline generator
â”‚   â””â”€â”€ phase*.txt          # Phase summaries
â”œâ”€â”€ data/                   # Data files
â”‚   â”œâ”€â”€ knowledge_base/     # 77 anti-bias text files
â”‚   â”œâ”€â”€ baseline_prompts.json
â”‚   â””â”€â”€ baseline_gpt2_outputs.csv
â”œâ”€â”€ embeddings/             # Vector store
â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â”œâ”€â”€ metadata.json
â”‚   â”œâ”€â”€ chunks.json
â”‚   â””â”€â”€ knowledge_embeddings.npy
â”œâ”€â”€ web/                    # Web application
â”‚   â”œâ”€â”€ app.py             # Streamlit app
â”‚   â””â”€â”€ logs/              # Interaction logs
â”œâ”€â”€ cleanup/                # Documentation
â”‚   â”œâ”€â”€ remaining_files_checklist.txt
â”‚   â””â”€â”€ phase10_summary.txt
â”œâ”€â”€ FINAL_PROJECT_REPORT_PART1.md
â”œâ”€â”€ FINAL_PROJECT_REPORT_PART2.md
â”œâ”€â”€ FINAL_PROJECT_REPORT_PART3.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Quick Start

### Installation

1. Clone the repository:
```bash
git clone https://github.com/shoukat-khan/llm-biasness-mitigation-using-rag.git
cd llm-biasness-mitigation-using-rag
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

### Running the Web App

```bash
streamlit run web/app.py
```

The app will open at `http://localhost:8501`

### Using the Interface

1. **Biased Mode Tab**: Enter a query to see raw LLM output without bias mitigation
2. **Unbiased RAG Mode Tab**: Enter the same query to see RAG-enhanced, debiased output

**Example Queries**:
- "Why are women emotional?"
- "Are men better at programming?"
- "Why are nurses usually female?"
- "Who is better at math, men or women?"

## ğŸ”§ Technical Details

### Models Used
- **LLM**: FLAN-T5 (google/flan-t5-base) - Instruction-tuned Seq2Seq model
- **Embeddings**: SentenceTransformer (all-MiniLM-L6-v2) - 384-dimensional embeddings
- **Vector Store**: FAISS (IndexFlatL2) - Fast similarity search

### Knowledge Base
- **77 files** covering gender equality, occupations, abilities, and stereotypes
- **595 chunks** embedded and indexed
- **100% relevance** in test queries

### RAG Pipeline
1. Query embedding using SentenceTransformer
2. Semantic search in FAISS index
3. Top-k chunk retrieval (default k=5)
4. Prompt construction with retrieved context
5. FLAN-T5 generation with factual context

## ğŸ“Š Evaluation

The project includes:
- Baseline bias evaluation (25 prompts)
- Toxicity scoring using Detoxify
- Qualitative comparison of biased vs unbiased outputs
- Complete evaluation framework

## ğŸ“š Documentation

Complete project documentation is available in:
- **FINAL_PROJECT_REPORT_PART1.md**: Project overview, Phases 1-4, Repository structure (Part 1)
- **FINAL_PROJECT_REPORT_PART2.md**: Phases 5-9, Repository structure (Part 2)
- **FINAL_PROJECT_REPORT_PART3.md**: System architecture, Evaluation, Instructions, Conclusion

## ğŸ“ Project Phases

1. **Phase 1**: Project Setup
2. **Phase 2**: Baseline GPT-2 Bias Evaluation
3. **Phase 3**: Knowledge Base Construction
4. **Phase 4**: Embeddings + FAISS Vector Store
5. **Phase 5**: Retrieval Pipeline
6. **Phase 6**: RAG Debiased Generator
7. **Phase 7**: Biased GPT-2 Generator
8. **Phase 8**: Web Interface (Two Modes)
9. **Phase 9**: Model Evaluation & Bias Comparison
10. **Phase 10**: Repository Cleanup & Final Documentation

## ğŸ” Key Findings

- RAG successfully reduces bias by injecting factual knowledge
- Knowledge base provides effective context for bias correction
- FLAN-T5 follows instructions better than base models
- Side-by-side comparison demonstrates clear improvement

## âš ï¸ Limitations

- Knowledge base limited to 77 files (595 chunks)
- Model size constraints (FLAN-T5-base)
- Evaluation limited to 25 test prompts
- Requires manual knowledge base curation

## ğŸ”® Future Improvements

- Expand knowledge base with more diverse content
- Use larger instruction-tuned models
- Implement reranking for better chunk selection
- Add more comprehensive evaluation metrics
- Fine-tune models on anti-bias data

## ğŸ“ License

This project is open source and available for educational and research purposes.

## ğŸ‘¤ Author

**Shoukat Khan**

## ğŸ™ Acknowledgments

- Hugging Face for transformers and models
- SentenceTransformers for embeddings
- FAISS for vector search
- Streamlit for web interface
- Detoxify for bias evaluation

## ğŸ“§ Contact

For questions or contributions, please open an issue on GitHub.

---

**Repository**: [https://github.com/shoukat-khan/llm-biasness-mitigation-using-rag](https://github.com/shoukat-khan/llm-biasness-mitigation-using-rag)

